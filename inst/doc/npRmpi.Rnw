%% $Id: entropy_np.Rnw,v 1.43 2010/02/18 14:43:37 jracine Exp jracine $

%\VignetteIndexEntry{Parallel np: Using the npRmpi Package}
%\VignetteDepends{npRmpi,boot,cubature,MASS}
%\VignetteKeywords{nonparametric, kernel, entropy, econometrics, qualitative,
%categorical}
%\VignettePackage{npRmpi}

\documentclass[nojss]{jss}

%% need no \usepackage{Sweave.sty}

\usepackage{amsmath,amsfonts}
\usepackage[utf8x]{inputenc} 

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\bbR}{\field{R}} %% Blackboard R
\newcommand{\bbS}{\field{S}} %% Blackboard S

\author{Jeffrey S.~Racine\\McMaster University}

\title{Parallel np: Using the npRmpi Package}

\Plainauthor{Jeffrey S.~Racine}

\Plaintitle{Parallel np: Using the npRmpi Package}

\Abstract{ 
  
  The \pkg{npRmpi} package is a parallel implementation of the
  \proglang{R} (\citet{R}) package \pkg{np} (\citet{np}). The
  underlying \proglang{C} code uses the message passing interface
  (`\proglang{MPI}') and is \proglang{MPI2} compliant.

}

\Keywords{nonparametric, semiparametric, kernel smoothing,
  categorical data}

\Plainkeywords{Nonparametric, kernel, econometrics, qualitative,
  categorical}

\Address{Jeffrey S.~Racine\\
  Department of Economics\\
  McMaster University\\
  Hamilton, Ontario, Canada, L8S 4L8\\
  E-mail: \email{racinej@mcmaster.ca}\\
  URL: \url{http://www.mcmaster.ca/economics/racine/}\\
}

\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%% Note - fragile using \label{} in \section{} - must be outside

%% For graphics

\setkeys{Gin}{width=\textwidth}

%% Achim's request for R prompt set invisibly at the beginning of the
%% paper

\section{Overview}

A common and understandable complaint often levied against
nonparametric kernel methods concerns the amount of computation time
required for data-driven bandwidth selection when one has a large data
set. There is a certain irony at play here since nonparametric methods
are ideally suited to situations involving large data sets, however,
computationally speaking their analysis may lie beyond the reach of
many users.  Some background may be in order. My co-authors and I
favor data-driven methods of bandwidth selection such as
cross-validation, among others. These bandwidth selection methods have
run times that are exponential in the number of observations (of
computational order $n^2$ hence a doubling of the sample size will
increase run time by a factor of four).

The solution adopted in the \pkg{npRmpi} package is to run the code in
a parallel computing environment and exploit the presence of multiple
processors when available. The underlying \proglang{C} code for \pkg{np}
is \proglang{MPI}-aware (\proglang{MPI} denotes the `message passing
interface', a popular parallel programming library that is an
international standard), and we merge the \pkg{R np} and \pkg{Rmpi}
packages to form the \pkg{npRmpi} package (this requires minor
modification of some of the underlying \pkg{Rmpi} code which is why we
cannot simply load the \pkg{Rmpi} package itself).\footnote{The
  \pkg{npRmpi} package incorporates the \pkg{Rmpi} package (Hao Yu
  <hyu@stats.uwo.ca>) with minor modifications and we are extremely
  grateful to Hao Yu for his contributions to the \proglang{R}
  community.}

All of the functions in \pkg{np} can exploit the presence of multiple
processors, and for large files run time is in general linear in the
number of processors present in that two processors will complete the
job in one half the amount of time that one processor
could.\footnote{There is minor overhead involved with message passing,
  and for small samples the overhead can be substantial when the ratio
  of message passing to computing the kernel estimator increases -
  this will be negligible for sufficiently large samples.}  Given the
availability of commodity cluster computers and the presence of
multiple cores in desktop and laptop machines, leveraging the
\pkg{npRmpi} package for large data sets may present a feasible
solution to the often lengthy computation times associated with
nonparametric kernel methods.

The code has been tested in the Mac OS X and Linux environments which
allow the user to compile \proglang{R} packages on the fly. Users
running MS Windows will have to consult local tech support personnel
and may also wish to consult the \pkg{Rmpi} package and web site for
assistance here. I cannot assist with installation issues beyond what
is provided in this document and trust the reader will forgive me for
this.

\section{Differences Between np and npRmpi}

There are only a few visible differences between running code in
serial versus parallel environments. Typically you run your parallel
code in batch mode so the first step would be to get your code running
in batch mode using the \pkg{np} package (obviously on a subset of
your data for large datasets). Once you have properly functioning
code, you will next add some `hooks' necessary for \proglang{MPI} to
run (see Section \ref{sec example} below for a detailed example), and
finally you will run the job using either \code{mpirun} or,
indirectly, via a batch scheduler on your cluster such as \code{sqsub}
(kindly consult your local support personnel for assistance on using
batch queueing systems on your local cluster).

\subsection*{Installation}

Installation will depend on your hardware and software configuration
and will vary widely across platforms. If you are not familiar with
parallel computing you are strongly advised to seek local advice.

That being said, if you have current versions of \proglang{R} and Open
\proglang{MPI} properly installed on your system, installation of the
\pkg{npRmpi} package could be as simple as downloading the
\pkg{npRmpi} tarball and, from a command shell, running

\begin{verbatim}
R CMD INSTALL npRmpi_foo.tar.gz 
\end{verbatim}

where foo is the version number. 

For clusters you may additionally need to provide locations of
libraries (kindly see your local sysadmin as there are far too many
variations for me to assist). On a local Linux cluster I use the
following by way of illustration (for this illustration we use the
Intel compiler suite and need to set \proglang{MPI} library paths and
\proglang{MPI} root directories):

\begin{verbatim}
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/sharcnet/openmpi/1.4.1/intel/lib
export MPI_ROOT=/opt/sharcnet/openmpi/1.4.1/intel 
R CMD INSTALL npRmpi_foo.tar.gz
\end{verbatim}

where again foo is the version number. 

Please seek local help for further assistance on installing and
running parallel programs.

\subsection{Parallel MPI Program Batch Execution}

To run a parallel \pkg{np} job having successfully installed the
\pkg{npRmpi} program, copy the \code{Rprofile} file in
\code{npRmpi/inst} to the current directory and name it
\code{.Rprofile} (or copy it to your home directory and again name it
\code{.Rprofile}).\footnote{You will need to download the \pkg{npRmpi}
  source code and unpack it in order to get Rprofile from the
  npRmpi/inst directory.} Then to run the job using two processors on
an Open \proglang{MPI} system you will enter something like

\begin{verbatim}
mpirun -np 2 R CMD BATCH npudens_npRmpi.R
\end{verbatim}

You can compare run times and any other differences by examining the
files \verb+npudens_serial.Rout+ and \verb+npudens_npRmpi.Rout+ (see
Section \ref{sec run time} below for some illustrative
examples). Clearly you could do this with a subset of your data for
large problems to judge the extent to which the parallel code reduces
run time.

If you have a batch scheduler installed on your cluster you might
instead enter something like

\begin{verbatim}
sqsub -q mpi -n 2 R CMD BATCH npudens_npRmpi.R
\end{verbatim}

Again, kindly consult local tech support personnel for issues
concerning the use of batch systems and cluster computing.

\subsection{Essential Program Elements}\label{sec example}

Here is a simple illustrative example of a serial batch program that
you would typically run using the \pkg{np} package.
\begin{verbatim}
## This is the serial version of npudens_npRmpi.R for comparison
## purposes (bandwidth ought to be identical, timing may
## differ). Study the differences between this file and its MPI
## counterpart for insight about your own problems.

library(np)
options(np.messages=FALSE)

## Generate some data

set.seed(42)
x <- rnorm(2500)

## A simple example with likelihood cross-validation

system.time(bw <- npudensbw(~x))

summary(bw)
\end{verbatim}

Below is the same code modified to run in parallel using the
\pkg{npRmpi} package. The salient differences are as follows:

\begin{enumerate}
  
  \item You {\em must} copy the \code{Rprofile} file from the npRmpi/inst
    directory of the tarball/zip file into either your root directory
    or current working directory and rename it \code{.Rprofile}. 
    
  \item You will notice that there are some \code{mpi.foo} commands
    where \code{foo} is, for example, \code{bcast.cmd}. These are the
    \pkg{Rmpi} commands for telling the slave nodes what to run.
      
      The first thing we do is initialize the master and slave nodes
      using the \code{np.mpi.initialize()} command.
      
    \item Next we broadcast our data to the slave nodes using the
      \code{mpi.bcast.Robj2slave()} command which sends an
      \proglang{R} object to the slaves.
      
    \item After this, we might compute the data-driven
      bandwidths. Note we have wrapped the \pkg{np} command
      \code{npudensbw()} in the \code{mpi.bcast.cmd()} with the option
      \code{caller.execute=TRUE} which indicates it is to execute on
      the master and slave nodes simultaneously.
      
    \item Finally, we clean up gracefully via the
      \code{mpi.close.Rslaves()} and \code{mpi.quit()} commands.
      
    \item There are a number of example files (including that above
      and below) in the \code{npRmpi/demo} directory that you may wish
      to examine. Each of these runs and has been deployed in a range
      of environments (Mac OS X, Linux).
        
\end{enumerate}


\begin{verbatim}
## Make sure you have the .Rprofile file from npRmpi/inst/ in your
## current directory or home directory. It is necessary.

## To run this on systems with OPENMPI installed and working, try
## mpirun -np 2 R CMD BATCH npudens_npRmpi. Check the time in the
## output file foo.Rout (the name of this file with extension .Rout),
## then try with, say, 4 processors and compare run time.

## Initialize master and slaves.

mpi.bcast.cmd(np.mpi.initialize(),
              caller.execute=TRUE)

## Turn off progress i/o as this clutters the output file (if you want
## to see search progress you can comment out this command)

mpi.bcast.cmd(options(np.messages=FALSE),
              caller.execute=TRUE)

## Generate some data and broadcast it to all slaves (it will be known
## to the master node)

set.seed(42)
x <- rnorm(2500)
mpi.bcast.Robj2slave(x)

## A simple example with likelihood cross-validation

system.time(mpi.bcast.cmd(bw <- npudensbw(~x),
                          caller.execute=TRUE))

summary(bw)

## Clean up properly then quit()

mpi.close.Rslaves()

mpi.quit()
\end{verbatim}

For more examples including regression, conditional density
estimation, and semiparametric models, see the files in the
\code{npRmpi/demo} directory.  Kindly study these files and the
comments in each in order to extend the parallel examples to your
specific problem.

Note that the output from the serial and parallel runs ought to be
identical save for execution time. If they are not there is a problem
with the underlying code and I would ask you to kindly report such
things to me immediately along with the offending code.

\section{Summary}

The \pkg{npRmpi} package is a parallel implementation of the \pkg{np}
package that can exploit the presence of multiple processors and the
\proglang{MPI} interface for parallel computing to reduce the
computational run time associated with kernel methods. Run time is
linear in the number of processors available, so two processors will
complete a job in roughly one half the time of one processor, ten in
one tenth and so forth.\footnote{There is minor overhead involved with
  message passing, and for small samples the overhead can be
  substantial as the ratio of message passing to computing the kernel
  estimator increases - this will be negligible for sufficiently large
  samples.} Though installation of a working \proglang{MPI}
implementation requires some familiarity with computer systems, local
expertise exists for many and help is to be found there. That being
said, the Mac OS X operating system comes stock with a fully
functioning version of Open \proglang{MPI} so there is zero additional
effort required for the user in order to get up and running in this
environment. Finally, any feedback for improvements for this document,
reporting of errors and bugs and so forth is always encouraged and
much appreciated.

\bibliography{npRmpi}

\appendix

\section{Illustrative Timed Runs}\label{sec run time}

The times reported in Table \ref{run table} were generated using R
2.11 and Open MPI 1.2.8 on a 2008 vintage MacBook running Snow Leopard
10.6.3 on a 2.4 GHz Intel Core 2 Duo (a completely stock
installation). Code was first run in serial mode using the np package
version 0.30-8 then in parallel mode with 2 processors using the
npRmpi package version 0.30-8. Elapsed time for the np functions is
provided (seconds) as is the ratio of the elapsed time for the
parallel run to the serial run.

Note that many of these illustrative examples use smallish sample
sizes hence the run time with 2 processors will not be 1/2 that with 1
processor due to overhead. But for larger samples (i.e. the ones you
actually need parallel computing for, not these toy illustrations) you
ought to see an improvement that is linear in the number of
processors.

\begin{table}
\begin{center}
  \caption{\label{run table}Illustrative timed runs (seconds) with 1
    processor (serial, np package) and 2 processors (parallel, npRmpi
    package).}
\begin{tabular}{lrrr}
Function & Secs(1) & Secs(2) & Ratio\cr
\hline
npcdens (cv.ls) & 111.5 & 125.0 & 0.89\cr
npcdens (cv.ml) & 27.3 & 48.8 & 0.56\cr
npcmstest & 332.3 & 592.4 & 0.56\cr
npconmode & 38.2 & 91.1 & 0.42\cr
npindex & 18.6 & 35.1 & 0.53\cr
npreg (lc) & 221.9 & 346.2 & 0.64\cr
npreg (ll) & 205.9 & 441.4 & 0.47\cr
npscoef & 31.6 & 38.3 & 0.82\cr
npsdeptest & 154.3 & 254.5 & 0.61\cr
npsigtest & 88.3 & 156.2 & 0.56\cr
npudens & 14.8 & 25.3 & 0.59\cr
\hline
\end{tabular}

\end{center}
\end{table}


\section{Known Issues}

\begin{enumerate}
  
\item It would be wise to cast all variables when read into
  \proglang{R} (always good practice) and not do so using the formula
  interface. 
  
  Casting responses (i.e.~stuff to the left of the \verb+~+) works in
  the serial version but does not appear to work for \verb+npcdensbw+.
  
\end{enumerate}

\end{document}
