%% $Id: entropy_np.Rnw,v 1.43 2010/02/18 14:43:37 jracine Exp jracine $

%\VignetteIndexEntry{Parallel np: Using the npRmpi Package}
%\VignetteDepends{npRmpi,boot,cubature,MASS}
%\VignetteKeywords{nonparametric, kernel, entropy, econometrics, qualitative,
%categorical}
%\VignettePackage{npRmpi}

\documentclass[nojss]{jss}

%% need no \usepackage{Sweave.sty}

\usepackage{amsmath,amsfonts}
\usepackage[utf8x]{inputenc} 

\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\N}{\field{N}}
\newcommand{\bbR}{\field{R}} %% Blackboard R
\newcommand{\bbS}{\field{S}} %% Blackboard S

\author{Jeffrey S.~Racine\\McMaster University}

\title{Parallel np: Using the npRmpi Package}

\Plainauthor{Jeffrey S.~Racine}

\Plaintitle{Parallel np: Using the npRmpi Package}

\Abstract{ 
  
  The \pkg{npRmpi} package is a parallel implementation of the
  \proglang{R} (\citet{R}) package \pkg{np} (\citet{np}). The
  underlying \proglang{C} code uses the \proglang{MPI} message passing
  interface and is \proglang{MPI2} compliant.

}

\Keywords{nonparametric, semiparametric, kernel smoothing,
  categorical data.}

\Plainkeywords{Nonparametric, kernel, econometrics, qualitative,
  categorical}

\Address{Jeffrey S.~Racine\\
  Department of Economics\\
  McMaster University\\
  Hamilton, Ontario, Canada, L8S 4L8\\
  E-mail: \email{racinej@mcmaster.ca}\\
  URL: \url{http://www.mcmaster.ca/economics/racine/}\\
}

\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%% Note - fragile using \label{} in \section{} - must be outside

%% For graphics

\setkeys{Gin}{width=\textwidth}

%% Achim's request for R prompt set invisibly at the beginning of the
%% paper

\section{Overview}

A common and understandable complaint often levied against
nonparametric kernel methods is the large amount of computation time
required for data-driven bandwidth selection when one has a large data
set. There is a certain irony at play here since nonparametric methods
are ideally suited to situations involving large data sets, however
computationally speaking, their analysis may lie beyond the reach of
many users.  Some background may be in order. Cross-validation
bandwidth selection methods have run times that are exponential in the
number of observations (of computational order $n^2$ hence a doubling
of the sample size will increase run time by a factor of four). 

The solution adopted in the \pkg{npRmpi} package is to run the code in
a parallel computing environment and exploit the presence of multiple
processors if available. The underlying \proglang{C} code for \pkg{np}
is \proglang{MPI}-aware (\proglang{MPI} denotes the `message passing
interface', a popular parallel programming library that is an
international standard), and we combine the \pkg{R np} and \pkg{Rmpi}
packages to form the \pkg{npRmpi} package (this requires some
modification to some of the underlying \pkg{Rmpi} code which is why we
cannot simply load the \pkg{Rmpi} package itself).

All of the functions in \pkg{np} can exploit the presence of multiple
processors, and run time is in general linear in the number of
processors present such that two processors will complete the job in
one half the amount of time that one processor could. Given the
availability of commodity cluster computers and the presence of
multiple cores in desktop and laptop machines, leveraging the
\pkg{npRmpi} package for large data sets may present a feasible
solution to the often lengthy computation times associated with
nonparametric kernel methods.

The code has been tested in the Mac OS X and Linux environments which
allow the user to compile \proglang{R} packages on the fly. Users
running MS Windows will have to consult their tech support personnel
and may also wish to consult the \pkg{Rmpi} package and web site for
assistance here. I cannot assist with installation issues beyond what
is provided in this document and trust the reader will forgive me for
this.

\section{Differences between np and npRmpi}

There are only a few visible differences between running code in
serial versus parallel environments. Typically you run your parallel
code in batch mode so the first step would be to get your code running
in serial mode using the \pkg{np} package (obviously on a subset of
your data). Once you have properly functioning code, you will next add
some `hooks' necessary for \proglang{MPI} to run, and finally you will
run the job using either \code{mpirun} or, indirectly, via a batch
scheduler on your cluster such as \code{sqsub}.

\subsection*{Installation}

Installation will depend on your hardware and software
configuration. If you are not familiar with parallel computing you
must seek local advice. 

That being said, if you have Open \proglang{MPI} and \proglang{MPI2}
properly installed on your system, installation could be as simple as
downloading the tarball and, from a command shell, running

\begin{verbatim}
R CMD INSTALL npRmpi_foo.tar.gz 
\end{verbatim}

where foo is the version number. 

For clusters you may additionally need to provide locations of
libraries (kindly see your local sysadmin as there are far too many
variations for me to assist). On a local Linux cluster I use the
following by way of illustration (we need to set \proglang{MPI}
library paths and \proglang{MPI} root directories):

\begin{verbatim}
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/sharcnet/openmpi/1.4.1/intel/lib
export MPI_ROOT=/opt/sharcnet/openmpi/1.4.1/intel 
R CMD INSTALL npRmpi_foo.tar.gz
\end{verbatim}

where again foo is the version number. 

Please seek local help for further assistance on installing and
running parallel programs.

\subsection{Parallel MPI Program Batch Execution}

To run the \proglang{MPI} version install the \pkg{npRmpi} program,
copy the \code{Rprofile} file in \code{npRmpi/inst} to the current
directory and name it \code{.Rprofile} (or copy it to your home
directory and again name it \code{.Rprofile}) and then on Open
\proglang{MPI} systems run something like

\begin{verbatim}
mpirun -np 2 R CMD BATCH npudens_npRmpi.R
\end{verbatim}

You can compare run times and any other differences by examining the
files \verb+npudens_serial.Rout+ and
\verb+npudens_npRmpi.Rout+. Clearly you could do this with a subset of
your data for large problems to judge the extent to which the parallel
code reduces run time.

\subsection{Essential Program Elements}

Here is a simple illustrative example of a serial batch program that
you would typically run using the \pkg{np} package.
\begin{verbatim}
## This is the serial version of npudens_npRmpi.R for comparison
## purposes (bandwidth ought to be identical, timing may
## differ). Study the differences between this file and its MPI
## counterpart for insight about your own problems.

library(np)
options(np.messages=FALSE)

## Generate some data

set.seed(42)
x <- rnorm(2500)

## A simple example with likelihood cross-validation

system.time(bw <- npudensbw(~x))

summary(bw)
\end{verbatim}

Below is the same code set up to run in parallel using the
\pkg{npRmpi} package. The salient differences are as follows:

\begin{enumerate}
  
  \item You {\em must} copy the \code{Rprofile} file from the npRmpi/inst
    directory of the tarball/zip file into either your root directory
    or current working directory and rename it \code{.Rprofile}. 
    
    \item You will notice that there are some \code{mpi.foo} commands
      where \code{foo} is, for example, \code{bcast.cmd}. These the
      the \pkg{Rmpi} commands for telling the slave nodes what to
      run. 
      
      The first thing we do is initialize the master and slave nodes
      using the \code{np.mpi.initialize()} command.
      
    \item Next we broadcast our data to the slave nodes using the
      \code{mpi.bcast.Robj2slave()} command which sends an
      \proglang{R} object to the slaves.
      
    \item After this, we might compute the data-driven
      bandwidths. Note we have wrapped the \pkg{np} command
      \code{npudensbw()} in the \code{mpi.bcast.cmd()} with the option
      \code{caller.execute=TRUE} which indicates it is to execute on
      the master and slave nodes simultaneously.
      
    \item Finally, we clean up gracefully via the
      \code{mpi.close.Rslaves()} and \code{mpi.quit()} commands.
      
    \item There are a number of example files (including that above
      and below) in the \code{npRmpi/demo} directory that you may wish
      to examine. Each of these runs and has been deployed in a range
      of environments (Mac OS X, Linux).
        
\end{enumerate}


\begin{verbatim}
## Make sure you have the .Rprofile file from npRmpi/inst/ in your
## current directory or home directory. It is necessary.

## To run this on systems with OPENMPI installed and working, try
## mpirun -np 2 R CMD BATCH npudens_npRmpi. Check the time in the
## output file foo.Rout (the name of this file with extension .Rout),
## then try with, say, 4 processors and compare run time.

## Initialize master and slaves.

mpi.bcast.cmd(np.mpi.initialize(),
              caller.execute=TRUE)

## Turn off progress i/o as this clutters the output file (if you want
## to see search progress you can comment out this command)

mpi.bcast.cmd(options(np.messages=FALSE),
              caller.execute=TRUE)

## Generate some data and broadcast it to all slaves (it will be known
## to the master node)

set.seed(42)
x <- rnorm(2500)
mpi.bcast.Robj2slave(x)

## A simple example with likelihood cross-validation

system.time(mpi.bcast.cmd(bw <- npudensbw(~x),
                          caller.execute=TRUE))

summary(bw)

## Clean up properly then quit()

mpi.close.Rslaves()

mpi.quit()
\end{verbatim}

For more examples including regression, conditional density
estimation, and semiparametric models, see the files in the
\code{npRmpi/demo} directory.  Kindly study these files and the
comments in each in order to extend the parallel examples to your
problem.

Note that the output from the serial and parallel runs ought to be
identical save for execution time. If they are not there is a problem
with the underlying code and I would ask you to kindly report such
things to me immediately along with the offending code.

\section{Summary}

The \pkg{npRmpi} package is a parallel implementation of the \pkg{np}
package that can exploit the presence of multiple processors and the
\proglang{MPI} interface for parallel computing to reduce the
computational run time associated with kernel methods. Run time is
linear in the number of processors available, so two processors will
complete a job in roughly one half the time of one processor, ten in
one tenth and so forth. Though installation of a working
\proglang{MPI} implementation requires some familiarity with computer
systems, local expertise exists for many and help is to be found
there. That being said, the Mac OS X operating system comes stock with
a fully functioning version of Open \proglang{MPI} so there is zero
additional effort required for the user in order to get up and running
in this environment. Finally, any feedback for improvements for this
document, reporting of errors and bugs and so forth is always
encouraged and much appreciated.

\bibliography{npRmpi}

\end{document}
